#GCP #Docker #PyTorch 

# はじめに

こんちは。   
新入社員の渡邉です。

要望が多かったので自作の機械学習エンジンをGCPにデプロイする方法についてまとめました。 

本記事の対象読者は、
駆け出しエンジニアや、プロジェクトマネージャー、データアナリストです。＜根拠になっていない＞

手早くアプリケーションを公開したい方に向けて分かりやすく丁寧に説明していくつもりです。  
不明点等ありましたら、コメント等で質問してください。  
あと[Twitter](https://twitter.com/panicyusuke)もフォローしてくださると嬉しいです。

## 自作の機械学習モデルをデプロイしたいとき、どうしますか？

近年、GCPやAWSを用いて事前トレーニング済みの機械学習エンジンをAPI経由で利用できるサービスが増えています。
これらはもちろんノーコードなので、プログラマーや専門知識がない方でもユースケースが一致していれば、その日中に利用することができるでしょう。

画像内の人の数を計測したり、不健康な植物を検知したいときは [AWS Amazon Rekognition](https://aws.amazon.com/jp/rekognition/?c=ml&sec=srv&blog-cards.sort-by=item.additionalFields.createdDate&blog-cards.sort-order=desc)を選択すると良いでしょう。

自分のTweetがどのような傾向なのか、感情分析やコンテンツ分類をして客観的に自分がどんな性格なのか知りたい？
それなら [GCP Natural Language API](https://cloud.google.com/natural-language/docs/basics?hl=ja) がオススメです。
標準的なREST API<RESTの説明を軽くする>を利用して、GCP上で推論・分析された結果を簡単に取得できます。

＜この辺もっとわかりやすく、初心者の方がアレルギーを起こさないように。＞

### これまでの実装方法

＜つながりが理解しづらい、もっと論理を強く、なぜWebフレームワークの説明をしているのかが意味不明＞

`Django`や`Flask`, `Bottle` などの適当なWebフレームワークと、 `Keras`や`Tensorflow`などの機械学習ライブラリを組み合わせて、`GCE`や`EC2` などのCloudサービスにデプロイするのも選択肢の一つだと思います。

とはいえ、機械学習エンジンに推論させたいだけなのにわざわざWebアプリの実装や、サーバーの設定を行うのはとても面倒です。
そして多くのデータアナリスト・機械学習エンジニアはWebアプリ開発より、モデルやアルゴリズムの最適化に時間をかけたいはずです...

そこで登場するのが`PyTorch`のモデルサービングライブラリである`TorchServe` です。

モデルサービングとは、名前のとおり自分で作成した機械学習モデルをAPIサーバーとして実行可能な状態にすることです。

モデルサービングについての詳細はAWSの記事が参考になります。
> モデルのデプロイプロセスを簡略化する 1 つの方法は、モデルサーバー、つまり、本番環境で機械学習予測を行うために特別に設計された既製のウェブアプリケーションを使用することです。モデルサーバーを使用すると、1 つまたは複数のモデルを簡単に読み込むことができ、スケーラブルなウェブサーバーに基づく予測 API が自動的に作成されます。また、予測リクエストに対して前処理と後処理のコードを実行することもできます。最後に忘れてならないのが、モデルサーバーは、ログ記録、モニタリング、セキュリティなどの生産に不可欠な機能も提供している点です。一般的なモデルサーバーには、[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) と [Multi Model Server](https://github.com/awslabs/multi-model-server) があります。

イメージとしてはWebフレームワークと機械学習ライブラリが一緒になったようなものです。
有名な機械学習のフレームワークには大体このモデルサービング用のライブラリがあります。
今回はPytorchのモデルサービングライブラリ、`TorchServe`を使って機械学習モデルサーバーを使って説明していきます！

### 注意

`TorchServe` が動作するかを検証するにはローカル環境で十分ですが、実際の運用では公開されたサーバー上にデプロイすることがほとんどだと思うので、本記事ではGCPの`Artifact Registry` と、`AI Platform Prediction`に機械学習モデルをデプロイするところまでを説明します。

また、タイトルにCPU版とあるように今回はGPUを使いません。
理由は、今回のサンプルで使うYolov5の推論のための計算量がそこまで大きくないからです。
嘘です、本音はGPUの設定は少しややこしく、ただでさえDocker, TorchServe, AI Platformなどの特殊な環境を使っているので説明が大変そうなので今回は割愛させていただきます。

(業務等ですぐに情報が欲しい方はコメントで質問していただけると補足します。)

### 対象読者

- Docker, PyTorch, 

### 動作環境

- OS: Ubuntu20.04 LTS
- CPU: AMD Ryzen 3700X
- gcloud SDK: 359.0.0
- Python: 3.9.2

## 本題

長くなりましたが、やっと本題です。
次の手順に沿って、自作機械学習エンジンをデプロイする方法を学びましょう！

1. Pythonで

### Python Library Version

```txt

# pip install -r requirements.txt  
  
matplotlib>=3.2.2  
numpy>=1.18.5  
opencv-python>=4.1.2  
Pillow
PyYAML>=5.3.1  
scipy>=1.4.1  
torch>=1.7.0  
torchvision>=0.8.1  
tqdm>=4.41.0  
```

### Requrements

- Python環境
- GoogleCloudPlatformアカウント
- Google Cloud SDKがインストールされているPC(詳細は[公式ドキュメント](https://cloud.google.com/sdk/docs/install?hl=JA#linux)を参考に進めてください)
- [Docker](https://www.docker.com/get-started)(Windowsの方は設定方法が異なります。こちらも公式ドキュメントを参考に進めてください)

`GPU用`
```bash

pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html

```

## 本記事で説明すること

- `TorchServe`
- `Artifact Registry`
- `AI Platform Prediction`

## 本記事で説明しないこと

- PyTorch
- Python

## TorchServeの仕組み

詳細は [TorchServe Prediction API(英語版)](https://pytorch.org/serve/inference_api.html#health-check-api) を参考に。
要約すると...

- 普段機械学習エンジンの実装で触れないところをサポートするよ
  - APIの構築
  - 複数リクエスト対応
  - 基本的なセキュリティはまかせて！
- [gRPC API](https://grpc.io/)でもRESTでもどんとこい
- JSONに基づいてRequset, Responseを定義するよ
- PyTorchで書いたコードをそのまま使えるよ

もっと知りたい人は[こちら](https://aws.amazon.com/jp/blogs/news/announcing-torchserve-an-open-source-model-server-for-pytorch/) でまとめてくれているので参考にしてください。

## Artifact Registry

公式より...
> Artifact Registry は、パッケージと Docker コンテナ イメージを 1 か所で保管し管理できる場所として機能します。

つまり、PythonやNodeなどの実行環境と、モデルをそのままDockerイメージとすることが出来るサービスである。

[公式ドキュメントより](https://cloud.google.com/artifact-registry/docs/overview?hl=ja)


## AI Platform Prediction

公式より...
> AI Platform Prediction は、TensorFlow、scikit-learn、XGBoost の能力と柔軟性をクラウドに導入します。AI Platform Prediction を使用して、トレーニング済みモデルをホストできるため、予測リクエストを送信できます。

名前から推測できるように、`AI Platform Train` もある。
`AI Platform Train`は、モデルの学習環境をGCP側でもってくれるもので、これまた便利なので別記事で書きます。

## TorchServeの実行

1. `Artifact Registry`の登録と作成
2. カスタムコンテナを`Artifact Registry` へ`PUSH`
3. T.B.D


### 参考

- [gcloud beta artifacts repositoriesコマンド](https://cloud.google.com/sdk/gcloud/reference/beta/artifacts/repositories)
- [Torchserveでモデルをデプロイしてカスタムハンドラーを作成します](https://ichi.pro/torchserve-de-moderu-o-depuroi-shite-kasutamuhandora-o-sakuseishimasu-277270280257169)