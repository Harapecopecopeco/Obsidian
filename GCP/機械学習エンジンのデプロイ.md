,#GCP #Docker #PyTorch 

# はじめに

自作の機械学習モデルをデプロイ（インターネット上に配置し、実行可能にすること）しようと思ったときどうしますか？
`Django`や`Flask`, `Bottle` などの適当なWebフレームワークと、 `Keras`や`Tensorflow`などの機械学習ライブラリを組み合わせて、`GCE`や`EC2` などのCloudサービスにデプロイするのも選択肢の一つだと思います。

しかし、機械学習エンジンに推論させたいだけなのにわざわざWebアプリの実装や、サーバーの設定を行うのはとても面倒です。

そこで登場するのがPyTorchのモデルサービングライブラリである`TorchServe` です。
`TorchServe` が動作するかを検証するにはローカル環境で十分ですが、実際の運用では公開されたサーバー上にデプロイすることがほとんどだと思うので、本記事ではGCPの`Artifact Registry` に機械学習モデルをデプロイするところまでを説明します。

⚠ `Artifact Registry` に関する説明は不十分な可能性があります。

## Requrements

- GCPアカウント
- Google Cloud SDKがインストールされているPC

## 本記事で説明すること

- `TorchServe`
- `Artifact Registry`

## 本記事で説明しないこと

- PyTorch
- Python

## TorchServeの仕組み

詳細は [TorchServe Prediction API(英語版)](https://pytorch.org/serve/inference_api.html#health-check-api) を参考に。
要約すると...

- 普段機械学習エンジンの実装で触れないところをサポートするよ
  - APIの構築
  - 保守
  - リクエスト量に応じたスケーリング
- [gRPC API](https://grpc.io/)ベース
- JSONに基づいてRequset, Responseを定義するよ
- PyTorchで書いたコードをそのまま使えるよ

もっと知りたい人は[こちら](https://aws.amazon.com/jp/blogs/news/announcing-torchserve-an-open-source-model-server-for-pytorch/) でまとめてくれているので参考にしてください。

## Artifact Registry

> Artifact Registry は、パッケージと Docker コンテナ イメージを 1 か所で保管し管理できる場所として機能します。

つまり、PythonやNodeなどの実行環境と、モデルをそのままDockerイメージとすることが出来るサービスである。

[公式ドキュメントより](https://cloud.google.com/artifact-registry/docs/overview?hl=ja)


## TorchServeの

1. `Artifact Registry`の登録と作成
2. カスタムコンテナを`Artifact Registry` へ`PUSH`


### 参考

- [gcloud beta artifacts repositoriesコマンド](https://cloud.google.com/sdk/gcloud/reference/beta/artifacts/repositories)
- [Torchserveでモデルをデプロイしてカスタムハンドラーを作成します](https://ichi.pro/torchserve-de-moderu-o-depuroi-shite-kasutamuhandora-o-sakuseishimasu-277270280257169)